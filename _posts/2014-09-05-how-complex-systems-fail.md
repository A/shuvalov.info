---
layout: post
title: "«Как рушатся комплексные системы», Ричард И. Кук"
description: "О фундаментальных проблемах больших запутанных систем"
keywords: ["Дизайн", "Design", "Книги", "Книга", "заметки"]
feature: true
cover: /assets/articles-assets/features/cook.jpg
---

## О статье

Эта статья о медицине, а не о программировании. В медицине огромное
количество сложных систем и огромная цена ошибок. Эти системы выходят за рамки
программного кода, его архитектуры, организации, инструментов, и, возможно, эти системы
выходят за рамки самой медицины.

Я думаю, что эту статью можно и полезно рассматривать в контексте
программирования. Очень интересно переносить выводы статьи на те
системы, которые мы разрабатываем: веб-сайты, различные сервисы,
приложения. Тут и правда есть над чем задуматься.

Надавно я прочитал книгу Митника «Искуство обмана», и многие приемы социальной
инжинерии, если их считать теми самыми катастрофами, о которых говорит Ричард Кук,
наглядно демонстрируют проблемы больших комплексных систем: несогласованность,
неверный пост-анализ ошибок, поиск «главной причины»…

![{{ page.title }}](/assets/articles-assets/cook.jpg)


## Как рушатся комплексные системы

### 1. Коплексные системы опасны по своей природе

> Все системы, которые мы рассматриваем (трансплантация, здравоохранение,
> энергетика) фактически и к сожалению, опасны по своей природе.
> Уровень риска периодически изменяется, но процессы, происходящие внутри этих
> систем не становятся безопаснее. Присутствие опасности заставляет создавать
> защиту от нее. Это отличительная черта комплексных систем.

### 2. Комплексные системы надежно защищены от ошибок

>  Тяжелые последствия ошибок со временем приводят к построению множества
> слоев для защиты от ошибок. Защита включает в себя различные технические
> компоненты (к примеру, систему резервного копирования,  различные
> предохранители), человеческие компоненты (такие, как тренинги и знания),
> а так же различную организационную и правовую защиту (различные политики,
> процедуры, сертификации, рабочие правила, командные тренинги). Все
> эти меры должны создать несколько щитов, для защиты от возможной
> катастрофы.

### 3. К катастрофе приводят несколько ошибок — одной не достаточно

> Эти щиты работают. В целом, система справляется. Катастрофические
> ошибки случаются когда небольшие, кажущиеся безобидными, ошибки
> объединяются, создавая шанс катастрофы. Каждая из этих ошибок
> важна, но только их совокупность приводит к трагедии. Шанс
> возникновения небольших ошибок гораздо выше, чем явной катастрофы.
> Большинство вероятных траекторий блокируется на уровне архитектуры
> безопасности компонентов системы. Те же траектории, которые
> ведут к инциденту, зачастую блокируются врачами.

### 4. Комплексные системы работают с ошибками, которые остаются незамеченными

> Сложность этих систем ведет к невозможности их функционирования
> без большого количества мелких трещин. Так как этих трещин не достаточно
> для возникновения серьезных ошибок, они рассматриваются, как
> незначительный фактор в процессе работы. Избавиться от всех
> возможных ошибок сложно по большей части из экономической стоимости,
> а также из-за невозможности без реальных фактов увидеть как
> могут объединиться эти ошибки для возникновения катастрофы.  Ошибки
> постоянно меняются, потому что меняются технологии, меняется организация
> работы и прилагаются усилия для  избавления от возможных
> ошибок.

### 5. Комплексные системы разрушаются в процессе работы

> Из предыдущего пункта выходит, что комплексные системы функционируют
> в сломанном состоянии. Системы продолжают работать из-за своей
> избыточности, из-за людей, которые могут заставить их работать
> не смотря на существующие ошибки. После анализа инцидента, практически
> всегда видно, что система имела «прото-инцидент»,  близкий к катастрофе.
> Аргументы о том, что условия, которые привели к деградации системы возможно
> было распознать перед происшествием обычно основываются на достаточно
> наивном представлении о работе системы. Система постоянно взаимодействует
> с различными компонентами (организационными, техническими, с людьми),
> которые ошибаются и изменяются постоянно.

### 6. Катастрофа происходит на пересечении

> Комплексные системы имеют потенциал катастрофической ошибки. Рядом с системами
> практически всегда находятся люди, и временная вероятность ошибки может
> привести к беде. Вероятность катастрофы — клеймо комплексных систем. Невозможно
> полностью избавиться от возможной трагедии. Вероятность возникновения
> ошибок заложена в самой природе в комплексных систем.

### 7. Сама идея понять «главную причину» не правильна

> Выделить основную причину невозможно — явные ошибки требуют множества
> деффектов. Проблема создается из множества отдельных частей, каждой
> из которых в отдельности не достаточно. Только соединившись вместе, они
> приводят к катастрофе. Важна именно эта связь. Таким образом, выделить
> «главную причину» не получится. Оценка, основанная на «главной причине»
> отражает не техническое понимание, основанное на самой природе ошибки,
> а, скорее, социальное, нуждающееся в искуственном определении и локализации
> сил или событий, которые привели к трагедии.


### 8. Пост-анализ работы людей после происшествий

> Знание результата приводят к тому, что произошедшие события начинают
> казаться гораздо более предсказуемыми, чем это было на самом деле. Выходит,
> что пост-фактум оценка, основывающаяся на анализе работы людей, не точна.
> Знания результата отравляют возможность после катастрофы воссоздать ситуацию
> в которой находились работники до трагедии. Кажется, что работники «должны
> были понимать», как ситуация «неминуемо» приближается к катастрофе.
> Взгляд в прошлое — одна из главных проблем в расследовании насчастного
> случая, особенно, когда к делу привлекаются эксперты в области
> человеческой работы.

### 9. Человеческая роль двойственна: он и разработчик и защитник от ошибок

> Люди используют систему для созданием того продукта, который они хотят получить,
> но кроме этого, их усилия так же направлены на избежание возможных ошибок.
> Динамика качества в системе основывается на балансировании требованиями к результату.
> Стороннему наблюдателю сложно увидеть двойственность этой роли: до ошибки
> особое внимание уделяется именно развитию продукта, тогда как после
> фокус переключется на защиту. В любой из этих моментов, сторонний наблюдатель
> ошибочно воспринимает работника системы, как постоянно сфокусированного
> на обоих ролях одновременно.

### 10. Все действия врачей — азартные игры

> После катастрофы явная ошибка зачастую кажется неизбежной, так же как и неверные
> действия или пренебрежение правилами, которые привели к трагедии.
> Но абсолютно все действия врачей — азартные игры, которые происходят в условиях
> неопределенности результатов. Степень неопределенности меняется в зависимости
> от обстоятельств. Авантюра в действих практикующих врачей заметна после
> трагического случая, причиной которого эту авантюру и называют. Но верно и
> обратное — успешные результаты так же являются разультатом авантюры. Эта
> сторона остается без внимания.

### 11. Действия — нож, раскрывающий неопределенность

> Организации неоднозначны, часто нарочно, в отношении к произврдительности,
> ресурсам, экономии, стоимости операций и приемлемом уровне риска больших
> и маленьких аварий. Эта неоднозначность разрешается действиями практиков
> на другом конце системы. После аварии, эти действия можно расценить как
> «ошибки» или «нарушения», но эта оценка сильно искажена ретроспективой и
> пренебрегает различными факторами, особенно, давлением в процессе работы.

### 12. Люди — адаптивный элемент комплексных систем

> Практикующие врачи и первое руководящее звено активно адаптируют систему
> для максимизации результатов и минимизации несчастных случаев. Эти адаптации
> происходят время от времени. Некоторые из адаптаций включают:
>
> 1. Реструктуризацию системы для ослабления уявимостей,
> 2. Концентрацию критического ресурса в зонах повышенного внимания.
> 3. Обеспечивание пути для отступления или восстановления в случае внезапных
> или ожидаемых сбоев.
> 4. Создание механизмов для быстрого обранужения изменений в производительности
> системы и плавного введения соответствующих ограничений или других способов
> повышения отказоустойчивости.

### 13. Экспертиза людей в сложной системе постоянно меняется

> Комплексные системы требуют большого опыта людей, работающих и управляющих ими.
> Уровень опыта изменяется как от изменения технологий, так и из-за необходимости
> искать замену тем экспертам, которые покидают систему. В любом случае,
> тренинги и оттачивание навыков — одна из функций систем. В любой момент времени,
> внутри системы находятся работники и тренеры с очень различным уровнем экспертизы.
> Серьезные проблемы в отношении экспертизы возникают из-за (1) необходимости
> использовать недостаточно высокий уровень опыта как ресурс для самых сложных
> или требовательных задач, (2) необходимости создавать опыт для использования
> в будущем.

### 14. Изменения приводят к новой форме ошибок

> Редкость действительно серьезных ошибок в надежных системах может вдохновлять
> на изменения, особенно на использование новых технологий, уменьшающих
> число незначительных, но часто возникающих ошибок. Эти изменения создают
> потенциальную возможность для новых, редких, но значимых катастроф. Когда
> новая технология испольуется для устранения хорошо понятых системных ошибок или
> для увеличения точности работы, она часто создает новые пути возникновения
> огромных, катастрофических ошибок. Не редко, эти новые редкие катастрофы
> создают больший удар, чем выгода от использования новой технологии. Эти новые
> формы ошибок сложно увидеть, до самого происшествия: внимание захватывают
> предполагаемые выгоды от новых изменений. Потому как новые серьезные ошибки
> происходят редко, внутри системы может произойти множество изменений до
> возникновения катастрофы, так что достаточно сложно заметить какой вклад
> в трагедию внесли эти изменения.

### 15. Оценка «причин» ограничивает эффективность борьбы с ошибками в будущем

> Меры, принимаемые после ошибок «из-за человеческого фактора» зачастую
> основываются на блокировании действий, которые могут стать «причиной»
> несчастного случая. Эти меры с самого края цепочки немного ослабляют вероятность
> подобного происшествия в будущем. Фактически, вероятность возникновения
> подобных ошибок колоссально мала, потому что набор латентных ошибок в системе
> изменяется постоянно. Вместо увеличения безопасности, эти меры обычно
> увеличивают сцепление и сложность внутри самой системы, что, в свою очередь,
> увеличивает количество латентных ошибок, присутствующих в системе, а также
> делают обнаружение и блокирование серьезных ошибок более запутанным и сложным.

### 16. Безопасность — это характеристика системы, а не отдельных ее компонентов

> Безопасность — это то свойство, которое появляется в результате работы системы.
> Она не находится в людях или отделах организаций и систем. Безопасность
> невозможно купить или изготовить. Это не функция, которую можно вынести
> в отдельную часть системы. С безопасностью нельзя обращаться как с сырьевым
> материалом. Состояние безопасности в любой системе всегда динамическое.
> Постоянные системные изменения для снижения опасности и управление этими
> изменениями постоянно меняются.

### 17. Люди постоянно создают безопасность

> Безошибочные операции — результат деятельности людей, которые работают над
> удержанием системы в нормальном режиме. Эта деятельность, по большему счету,
> часть нормальной работы, которая свиду достаточно проста. Но, потому как
> система никогда не функционирует без ошибок, люди адаприруются под измененяющиеся
> условия и создают безопасную среду от момента к моменту. Эта адаптация,
> в основном, сводятся к выбору хорошо отточенных действий среди возможных
> вариантов. Но иногда адаптация — это новая комбинация или создание совершенно
> нового подхода.

### 18. Безошибочная деятельность требует экспертизы в ошибках

> Определение опасностей и успешное управление работой системы для того, чтобы
> она оставалась в режиме нормального функционирования, все это требует
> близкого знакомства с ошибками. Более надежная работа системы обычно возникает
> когда рабочие могут различить «крайнюю точку огибающей». Это тот момент, когда
> работа в системе начинает ухудшаться, становится сложнее делать прогнозы
> и невозможно все быстро привести в порядок. Понимая опасность, люди, работающие
> системе, готовы к неожиданностям и хорошо оценивают опасность того пути,
> которым они идут к желаемому уровню производительности. Высокий уровень безопасности
> зависит от людей с откалиброванной оценкой рисков, а так же от самого процесса
> калибровки действиями, и от понимания того, как эти действия меняют
> огибающую.

Подписывайтесь на [РСС](http://feeds.feedburner.com/anton-shuvalov/FJHar).

[1]: http://www.farnamstreetblog.com/2014/04/antifragile-a-definition/
[2]: http://www.ctlab.org/documents/How%20Complex%20Systems%20Fail.pdf
